{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rais\\AppData\\Local\\Temp\\ipykernel_10796\\2501412702.py:9: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  db=FAISS.from_documents(documents[:30],OllamaEmbeddings(model=\"nomic-embed-text\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "docs = loader.load()\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db=FAISS.from_documents(documents[:30],OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "query=\"An attention function can be described as mapping a query \"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rais\\AppData\\Local\\Temp\\ipykernel_10796\\2952145899.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"gemma3\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Okay, let’s break down Scaled Dot-Product Attention based on the provided context.\\n\\n**Step 1: Core Idea**\\n\\nScaled Dot-Product Attention is a mechanism that computes relationships between different parts of an input sequence. It does this by comparing \"queries\" with \"keys\" to determine how much attention should be paid to each corresponding \"value.\"\\n\\n**Step 2: Input and Dimensions**\\n\\n*   **Queries (Q):**  Queries have a dimension of *dk*.\\n*   **Keys (K):** Keys also have a dimension of *dk*.\\n*   **Values (V):** Values have a dimension of *dv*.\\n\\n**Step 3: The Calculation**\\n\\n1.  **Dot Products:** The query (Q) is multiplied (dot product) with all the keys (K). This results in a matrix of scores representing the compatibility between the query and each key.\\n2.  **Scaling:** These dot products are then divided by the square root of *dk* (√*dk*). This scaling is crucial.\\n3.  **Softmax:** A softmax function is applied to the scaled dot products. This converts the scores into probabilities, representing the weights assigned to each value.\\n4.  **Weighted Sum:** Finally, the softmax output is used to compute a weighted sum of the values (V). The result is the attention output.\\n\\n**Step 4: The Formula**\\n\\nThe entire process can be summarized by the following formula:\\n\\nAttention(Q, K, V) = softmax(QKT√dk)V\\n\\n**Step 5: Why Scaling?**\\n\\nThe scaling factor of √*dk* is used to counteract a potential issue.  When *dk* is large, the dot products can become very large in magnitude. This can push the softmax function into regions where it has extremely small gradients, hindering learning. Scaling prevents this problem.\\n\\n**Step 6: Relationship to Other Attention Types**\\n\\nDot-product attention is identical to Scaled Dot-Product Attention, except for the scaling factor. Additive attention, in contrast, uses a feed-forward network with a single hidden layer to compute the compatibility function.\\n\\n**In essence, Scaled Dot-Product Attention is a fast and efficient way to model relationships within data, primarily utilized in the Transformer architecture.**\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm=Ollama(model=\"gemma3\")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "retriever=db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "response=retrieval_chain.invoke({\"input\":\"Scaled Dot-Product Attention\"})\n",
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
